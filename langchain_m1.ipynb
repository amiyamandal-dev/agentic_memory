{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76be286d8c13ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T05:38:27.113396Z",
     "start_time": "2025-07-21T05:38:26.236443Z"
    }
   },
   "outputs": [],
   "source": [
    "# !uv pip install -U langchain-deepseek psycopg2-binary pgvector langgraph langchain_deepseek chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590d0255caca4c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T05:37:52.570257Z",
     "start_time": "2025-07-21T05:37:52.519972Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Agentic AI Memory System with ChromaDB Vector Store and PostgreSQL\n",
    "Hybrid implementation using ChromaDB for vectors and PostgreSQL for structured data\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import logging\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union, TypedDict, Annotated, Callable, Set, Tuple\n",
    "from dataclasses import dataclass\n",
    "import operator\n",
    "import asyncio\n",
    "from enum import Enum\n",
    "\n",
    "# Database imports\n",
    "try:\n",
    "    from sqlalchemy import create_engine, Column, Integer, String, DateTime, Float, Text, ForeignKey, Index, Boolean\n",
    "    from sqlalchemy.dialects.postgresql import JSONB, UUID, ARRAY\n",
    "    from sqlalchemy.orm import declarative_base, sessionmaker, relationship, Session\n",
    "    from sqlalchemy import func, text, and_, or_, select\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Database dependencies required: pip install sqlalchemy psycopg2-binary. Error: {e}\")\n",
    "\n",
    "# Vector database imports\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    from chromadb.utils import embedding_functions\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"ChromaDB required: pip install chromadb. Error: {e}\")\n",
    "\n",
    "# ML/Embedding imports\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"ML dependencies required: pip install sentence-transformers numpy. Error: {e}\")\n",
    "\n",
    "# LangChain imports\n",
    "try:\n",
    "    from langchain_core.memory import BaseMemory\n",
    "    from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "    from langchain.schema.retriever import BaseRetriever\n",
    "    from langchain.schema.document import Document\n",
    "    from langchain_core.language_models.base import BaseLanguageModel\n",
    "    from langchain.tools import BaseTool, tool\n",
    "    from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain_core.output_parsers import JsonOutputParser\n",
    "    from langchain.callbacks.base import BaseCallbackHandler\n",
    "    from pydantic import BaseModel, Field\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"LangChain required: pip install langchain langchain-core langchain-community. Error: {e}\")\n",
    "\n",
    "# LangGraph imports\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END, START\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"LangGraph required: pip install langgraph. Error: {e}\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Memory type definitions with clear persistence rules\n",
    "PERSISTENT_MEMORY_TYPES: Set[str] = {\n",
    "    \"fact\",           # Factual information - always persistent\n",
    "    \"knowledge\",      # Domain knowledge - always persistent\n",
    "    \"reflection\",     # Agent insights - persistent for learning\n",
    "    \"goal\",          # Long-term goals - persistent\n",
    "    \"learned_skill\", # Learned capabilities - persistent\n",
    "    \"user_preference\" # User preferences - persistent\n",
    "}\n",
    "\n",
    "TEMPORARY_MEMORY_TYPES: Set[str] = {\n",
    "    \"conversation\",   # Recent dialogue - temporary\n",
    "    \"scratch\",       # Working memory - very temporary\n",
    "    \"plan\",          # Current plans - temporary\n",
    "    \"temporary\",     # Explicitly temporary\n",
    "    \"task\",          # Current tasks - temporary unless marked\n",
    "    \"observation\"    # Observations - temporary unless important\n",
    "}\n",
    "\n",
    "class MemoryImportance(Enum):\n",
    "    \"\"\"Memory importance levels\"\"\"\n",
    "    CRITICAL = 0.9\n",
    "    HIGH = 0.7\n",
    "    MEDIUM = 0.5\n",
    "    LOW = 0.3\n",
    "    MINIMAL = 0.1\n",
    "\n",
    "# Database Models (PostgreSQL) - No vector column needed\n",
    "class AgentMemory(Base):\n",
    "    \"\"\"Memory table for structured data (vectors stored in ChromaDB)\"\"\"\n",
    "    __tablename__ = 'agent_memories'\n",
    "\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    session_id = Column(String(255), nullable=False, index=True)\n",
    "    agent_id = Column(String(255), nullable=False, index=True, default=\"default_agent\")\n",
    "    memory_type = Column(String(50), nullable=False, index=True)\n",
    "    content = Column(Text, nullable=False)\n",
    "    summary = Column(Text)\n",
    "    \n",
    "    # ChromaDB will store the vector with this ID\n",
    "    \n",
    "    importance = Column(Float, default=0.5, index=True)\n",
    "    confidence = Column(Float, default=1.0)\n",
    "    access_count = Column(Integer, default=0)\n",
    "    last_accessed = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))\n",
    "    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc), index=True)\n",
    "    updated_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc),\n",
    "                        onupdate=lambda: datetime.now(timezone.utc))\n",
    "    \n",
    "    # Memory lifecycle fields\n",
    "    is_persistent = Column(Boolean, default=False, index=True)\n",
    "    expires_at = Column(DateTime(timezone=True), nullable=True, index=True)\n",
    "    is_summarized = Column(Boolean, default=False, index=True)\n",
    "    \n",
    "    meta_data = Column(JSONB, default=dict)\n",
    "    tags = Column(ARRAY(String), default=list)\n",
    "    \n",
    "    # Relationships\n",
    "    relationships_out = relationship(\"MemoryRelationship\", foreign_keys=\"MemoryRelationship.source_id\",\n",
    "                                     back_populates=\"source\", cascade=\"all, delete-orphan\")\n",
    "    relationships_in = relationship(\"MemoryRelationship\", foreign_keys=\"MemoryRelationship.target_id\",\n",
    "                                    back_populates=\"target\", cascade=\"all, delete-orphan\")\n",
    "\n",
    "    __table_args__ = (\n",
    "        Index('ix_memory_agent_session', 'agent_id', 'session_id'),\n",
    "        Index('ix_memory_importance_confidence', 'importance', 'confidence'),\n",
    "        Index('ix_memory_tags', 'tags', postgresql_using='gin'),\n",
    "        Index('ix_memory_persistence', 'is_persistent', 'expires_at'),\n",
    "    )\n",
    "\n",
    "class MemoryRelationship(Base):\n",
    "    \"\"\"Memory relationships\"\"\"\n",
    "    __tablename__ = 'memory_relationships'\n",
    "\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    source_id = Column(UUID(as_uuid=True), ForeignKey('agent_memories.id', ondelete='CASCADE'), nullable=False)\n",
    "    target_id = Column(UUID(as_uuid=True), ForeignKey('agent_memories.id', ondelete='CASCADE'), nullable=False)\n",
    "    relationship_type = Column(String(50), nullable=False)\n",
    "    strength = Column(Float, default=1.0)\n",
    "    confidence = Column(Float, default=1.0)\n",
    "    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))\n",
    "    meta_data = Column(JSONB, default=dict)\n",
    "\n",
    "    source = relationship(\"AgentMemory\", foreign_keys=[source_id], back_populates=\"relationships_out\")\n",
    "    target = relationship(\"AgentMemory\", foreign_keys=[target_id], back_populates=\"relationships_in\")\n",
    "\n",
    "class AgentTask(Base):\n",
    "    \"\"\"Table to track agent tasks and goals\"\"\"\n",
    "    __tablename__ = 'agent_tasks'\n",
    "\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    agent_id = Column(String(255), nullable=False, index=True)\n",
    "    session_id = Column(String(255), nullable=False, index=True)\n",
    "    task_description = Column(Text, nullable=False)\n",
    "    task_type = Column(String(50), nullable=False)\n",
    "    status = Column(String(20), default=\"pending\")\n",
    "    priority = Column(Float, default=0.5)\n",
    "    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))\n",
    "    completed_at = Column(DateTime(timezone=True), nullable=True)\n",
    "    result = Column(JSONB, default=dict)\n",
    "    meta_data = Column(JSONB, default=dict)\n",
    "\n",
    "class AgentReflection(Base):\n",
    "    \"\"\"Table for agent self-reflection and learning\"\"\"\n",
    "    __tablename__ = 'agent_reflections'\n",
    "\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    agent_id = Column(String(255), nullable=False, index=True)\n",
    "    session_id = Column(String(255), nullable=False, index=True)\n",
    "    reflection_type = Column(String(50), nullable=False)\n",
    "    content = Column(Text, nullable=False)\n",
    "    insights = Column(JSONB, default=list)\n",
    "    action_items = Column(JSONB, default=list)\n",
    "    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))\n",
    "    meta_data = Column(JSONB, default=dict)\n",
    "\n",
    "# Tool Input Models for LangChain\n",
    "class RememberFactInput(BaseModel):\n",
    "    \"\"\"Input for remember_fact tool\"\"\"\n",
    "    content: str = Field(description=\"The fact or information to remember\")\n",
    "    importance: float = Field(default=0.5, description=\"Importance level (0.0-1.0)\")\n",
    "    tags: List[str] = Field(default_factory=list, description=\"Tags for categorization\")\n",
    "    is_persistent: bool = Field(default=True, description=\"Whether to persist this memory\")\n",
    "\n",
    "class RecallMemoriesInput(BaseModel):\n",
    "    \"\"\"Input for recall_memories tool\"\"\"\n",
    "    query: str = Field(description=\"Search query for finding memories\")\n",
    "    memory_types: Optional[List[str]] = Field(default=None, description=\"Types of memories to search\")\n",
    "    limit: int = Field(default=5, description=\"Maximum number of results\")\n",
    "    min_score: float = Field(default=0.3, description=\"Minimum similarity score\")\n",
    "\n",
    "class ReflectPerformanceInput(BaseModel):\n",
    "    \"\"\"Input for reflect_on_performance tool\"\"\"\n",
    "    topic: str = Field(description=\"Topic of reflection\")\n",
    "    insights: List[str] = Field(default_factory=list, description=\"Key insights\")\n",
    "    action_items: List[str] = Field(default_factory=list, description=\"Action items\")\n",
    "\n",
    "class ChromaDBMemoryManager:\n",
    "    \"\"\"Hybrid memory manager using ChromaDB for vectors and PostgreSQL for data\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 database_url: str, \n",
    "                 agent_id: str = \"default_agent\",\n",
    "                 embedding_model_name: str = 'all-MiniLM-L6-v2',\n",
    "                 chroma_persist_directory: str = \"./chroma_db\",\n",
    "                 chroma_host: Optional[str] = None,\n",
    "                 chroma_port: Optional[int] = None):\n",
    "        \"\"\"Initialize with ChromaDB and PostgreSQL\"\"\"\n",
    "        self.agent_id = agent_id\n",
    "\n",
    "        try:\n",
    "            # Initialize PostgreSQL\n",
    "            self.engine = create_engine(\n",
    "                database_url,\n",
    "                echo=False,\n",
    "                pool_pre_ping=True,\n",
    "                pool_recycle=3600,\n",
    "                pool_size=10,\n",
    "                max_overflow=20\n",
    "            )\n",
    "\n",
    "            # Test connection\n",
    "            with self.engine.connect() as conn:\n",
    "                conn.execute(text(\"SELECT 1\"))\n",
    "\n",
    "            self.SessionLocal = sessionmaker(bind=self.engine, expire_on_commit=False)\n",
    "\n",
    "            # Initialize embedding model\n",
    "            self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "            # Initialize ChromaDB\n",
    "            if chroma_host and chroma_port:\n",
    "                # Use ChromaDB server\n",
    "                self.chroma_client = chromadb.HttpClient(\n",
    "                    host=chroma_host,\n",
    "                    port=chroma_port,\n",
    "                    settings=Settings(\n",
    "                        anonymized_telemetry=False,\n",
    "                        allow_reset=True\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Use persistent local ChromaDB\n",
    "                self.chroma_client = chromadb.PersistentClient(\n",
    "                    path=chroma_persist_directory,\n",
    "                    settings=Settings(\n",
    "                        anonymized_telemetry=False,\n",
    "                        allow_reset=True\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Create or get collection for this agent\n",
    "            collection_name = f\"agent_{agent_id}_memories\"\n",
    "            try:\n",
    "                self.collection = self.chroma_client.get_collection(\n",
    "                    name=collection_name,\n",
    "                    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                        model_name=embedding_model_name\n",
    "                    )\n",
    "                )\n",
    "                logger.info(f\"Using existing ChromaDB collection: {collection_name}\")\n",
    "            except:\n",
    "                self.collection = self.chroma_client.create_collection(\n",
    "                    name=collection_name,\n",
    "                    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                        model_name=embedding_model_name\n",
    "                    ),\n",
    "                    metadata={\"hnsw:space\": \"cosine\"}\n",
    "                )\n",
    "                logger.info(f\"Created new ChromaDB collection: {collection_name}\")\n",
    "\n",
    "            # Create PostgreSQL tables\n",
    "            self._create_tables()\n",
    "            self._setup_database_functions()\n",
    "\n",
    "            logger.info(f\"ChromaDBMemoryManager initialized for agent: {agent_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize ChromaDBMemoryManager: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_tables(self):\n",
    "        \"\"\"Create PostgreSQL tables\"\"\"\n",
    "        try:\n",
    "            Base.metadata.create_all(self.engine)\n",
    "            logger.info(\"All PostgreSQL tables created/verified\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create tables: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_database_functions(self):\n",
    "        \"\"\"Setup PostgreSQL functions for optimized operations\"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            # Function for batch memory cleanup\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE OR REPLACE FUNCTION cleanup_expired_memories()\n",
    "                RETURNS int AS $$\n",
    "                DECLARE\n",
    "                    deleted_count int;\n",
    "                BEGIN\n",
    "                    DELETE FROM agent_memories\n",
    "                    WHERE expires_at IS NOT NULL \n",
    "                        AND expires_at < NOW()\n",
    "                        AND is_persistent = false;\n",
    "                    \n",
    "                    GET DIAGNOSTICS deleted_count = ROW_COUNT;\n",
    "                    RETURN deleted_count;\n",
    "                END;\n",
    "                $$ LANGUAGE plpgsql;\n",
    "            \"\"\"))\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "    def create_memory(self, session_id: str, memory_type: str, content: str,\n",
    "                      importance: float = 0.5, meta_data: Optional[Dict] = None,\n",
    "                      summary: Optional[str] = None, tags: List[str] = None,\n",
    "                      confidence: float = 1.0, is_persistent: Optional[bool] = None) -> str:\n",
    "        \"\"\"Create memory with vector in ChromaDB and data in PostgreSQL\"\"\"\n",
    "\n",
    "        # Validation\n",
    "        if not all([session_id, memory_type, content]):\n",
    "            raise ValueError(\"session_id, memory_type, and content are required\")\n",
    "\n",
    "        session = self.SessionLocal()\n",
    "        try:\n",
    "            # Determine persistence based on memory type if not specified\n",
    "            if is_persistent is None:\n",
    "                is_persistent = memory_type in PERSISTENT_MEMORY_TYPES\n",
    "\n",
    "            # Calculate expiration for temporary memories\n",
    "            expires_at = None\n",
    "            if not is_persistent:\n",
    "                if memory_type == \"scratch\":\n",
    "                    expires_at = datetime.now(timezone.utc) + timedelta(hours=1)\n",
    "                elif memory_type == \"conversation\":\n",
    "                    expires_at = datetime.now(timezone.utc) + timedelta(days=7)\n",
    "                elif memory_type == \"plan\":\n",
    "                    expires_at = datetime.now(timezone.utc) + timedelta(days=3)\n",
    "                else:\n",
    "                    expires_at = datetime.now(timezone.utc) + timedelta(days=30)\n",
    "\n",
    "            # Create memory record in PostgreSQL\n",
    "            memory = AgentMemory(\n",
    "                session_id=str(session_id),\n",
    "                agent_id=self.agent_id,\n",
    "                memory_type=str(memory_type),\n",
    "                content=str(content),\n",
    "                summary=summary or (content[:97] + \"...\" if len(content) > 100 else content),\n",
    "                importance=float(importance),\n",
    "                confidence=float(confidence),\n",
    "                is_persistent=is_persistent,\n",
    "                expires_at=expires_at,\n",
    "                meta_data=meta_data or {},\n",
    "                tags=tags or []\n",
    "            )\n",
    "\n",
    "            session.add(memory)\n",
    "            session.commit()\n",
    "            \n",
    "            memory_id = str(memory.id)\n",
    "\n",
    "            # Add to ChromaDB with metadata\n",
    "            self.collection.add(\n",
    "                ids=[memory_id],\n",
    "                documents=[content],\n",
    "                metadatas=[{\n",
    "                    \"memory_id\": memory_id,\n",
    "                    \"agent_id\": self.agent_id,\n",
    "                    \"session_id\": session_id,\n",
    "                    \"memory_type\": memory_type,\n",
    "                    \"importance\": importance,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"is_persistent\": is_persistent,\n",
    "                    \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"tags\": json.dumps(tags or [])\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Created {'persistent' if is_persistent else 'temporary'} memory {memory_id} in both stores\")\n",
    "            return memory_id\n",
    "\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            logger.error(f\"Failed to create memory: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    def semantic_search(self, query: str, session_id: Optional[str] = None,\n",
    "                        memory_types: Optional[List[str]] = None, limit: int = 10,\n",
    "                        min_score: float = 0.0, include_agent_only: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantic search using ChromaDB with PostgreSQL data enrichment\"\"\"\n",
    "\n",
    "        if not query:\n",
    "            raise ValueError(\"Query is required\")\n",
    "\n",
    "        try:\n",
    "            # Build where clause for ChromaDB\n",
    "            where_clause = {\"agent_id\": self.agent_id} if include_agent_only else {}\n",
    "            \n",
    "            if session_id:\n",
    "                where_clause[\"session_id\"] = session_id\n",
    "            \n",
    "            if memory_types:\n",
    "                where_clause[\"memory_type\"] = {\"$in\": memory_types}\n",
    "\n",
    "            # Search in ChromaDB\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=limit * 2,  # Get more results for filtering\n",
    "                where=where_clause,\n",
    "                include=[\"metadatas\", \"distances\", \"documents\"]\n",
    "            )\n",
    "\n",
    "            if not results['ids'] or not results['ids'][0]:\n",
    "                return []\n",
    "\n",
    "            # Convert distances to similarity scores (1 - distance for cosine)\n",
    "            similarity_scores = [1 - d for d in results['distances'][0]]\n",
    "            \n",
    "            # Filter by minimum score\n",
    "            filtered_results = []\n",
    "            memory_ids = []\n",
    "            \n",
    "            for idx, (memory_id, score) in enumerate(zip(results['ids'][0], similarity_scores)):\n",
    "                if score >= min_score:\n",
    "                    memory_ids.append(memory_id)\n",
    "                    filtered_results.append({\n",
    "                        'memory_id': memory_id,\n",
    "                        'similarity_score': score,\n",
    "                        'metadata': results['metadatas'][0][idx]\n",
    "                    })\n",
    "\n",
    "            if not memory_ids:\n",
    "                return []\n",
    "\n",
    "            # Fetch full memory data from PostgreSQL\n",
    "            session = self.SessionLocal()\n",
    "            try:\n",
    "                memories = session.query(AgentMemory).filter(\n",
    "                    AgentMemory.id.in_([uuid.UUID(mid) for mid in memory_ids])\n",
    "                ).all()\n",
    "\n",
    "                # Create memory lookup\n",
    "                memory_dict = {str(m.id): m for m in memories}\n",
    "\n",
    "                # Combine ChromaDB and PostgreSQL data\n",
    "                final_results = []\n",
    "                for result in filtered_results:\n",
    "                    memory_id = result['memory_id']\n",
    "                    if memory_id in memory_dict:\n",
    "                        memory = memory_dict[memory_id]\n",
    "                        \n",
    "                        # Update access statistics\n",
    "                        memory.access_count += 1\n",
    "                        memory.last_accessed = datetime.now(timezone.utc)\n",
    "                        \n",
    "                        final_results.append({\n",
    "                            'id': memory_id,\n",
    "                            'content': memory.content,\n",
    "                            'summary': memory.summary,\n",
    "                            'memory_type': memory.memory_type,\n",
    "                            'importance': memory.importance,\n",
    "                            'confidence': memory.confidence,\n",
    "                            'similarity_score': result['similarity_score'],\n",
    "                            'tags': memory.tags,\n",
    "                            'created_at': memory.created_at.isoformat(),\n",
    "                            'access_count': memory.access_count,\n",
    "                            'meta_data': memory.meta_data\n",
    "                        })\n",
    "\n",
    "                session.commit()\n",
    "\n",
    "                # Sort by combined score\n",
    "                final_results.sort(\n",
    "                    key=lambda x: x['similarity_score'] * x['importance'] * x['confidence'],\n",
    "                    reverse=True\n",
    "                )\n",
    "\n",
    "                return final_results[:limit]\n",
    "\n",
    "            finally:\n",
    "                session.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Semantic search failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def delete_memory(self, memory_id: str) -> bool:\n",
    "        \"\"\"Delete memory from both ChromaDB and PostgreSQL\"\"\"\n",
    "        session = self.SessionLocal()\n",
    "        try:\n",
    "            # Delete from PostgreSQL (cascade will handle relationships)\n",
    "            memory = session.query(AgentMemory).filter(\n",
    "                AgentMemory.id == uuid.UUID(memory_id),\n",
    "                AgentMemory.agent_id == self.agent_id\n",
    "            ).first()\n",
    "\n",
    "            if memory:\n",
    "                session.delete(memory)\n",
    "                session.commit()\n",
    "\n",
    "                # Delete from ChromaDB\n",
    "                self.collection.delete(ids=[memory_id])\n",
    "                \n",
    "                logger.info(f\"Deleted memory {memory_id} from both stores\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            logger.error(f\"Failed to delete memory: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    def apply_forgetting_policy(self) -> int:\n",
    "        \"\"\"Apply forgetting policy to both stores\"\"\"\n",
    "        session = self.SessionLocal()\n",
    "        try:\n",
    "            # Get expired and low-value memories\n",
    "            expired_memories = session.query(AgentMemory).filter(\n",
    "                AgentMemory.agent_id == self.agent_id,\n",
    "                AgentMemory.is_persistent == False,\n",
    "                or_(\n",
    "                    and_(\n",
    "                        AgentMemory.expires_at != None,\n",
    "                        AgentMemory.expires_at < datetime.now(timezone.utc)\n",
    "                    ),\n",
    "                    and_(\n",
    "                        AgentMemory.access_count == 0,\n",
    "                        AgentMemory.created_at < datetime.now(timezone.utc) - timedelta(days=1),\n",
    "                        (AgentMemory.importance * AgentMemory.confidence) < 0.3\n",
    "                    )\n",
    "                )\n",
    "            ).all()\n",
    "\n",
    "            memory_ids = [str(m.id) for m in expired_memories]\n",
    "            \n",
    "            if memory_ids:\n",
    "                # Delete from PostgreSQL\n",
    "                for memory in expired_memories:\n",
    "                    session.delete(memory)\n",
    "                \n",
    "                session.commit()\n",
    "\n",
    "                # Delete from ChromaDB in batches\n",
    "                batch_size = 100\n",
    "                for i in range(0, len(memory_ids), batch_size):\n",
    "                    batch = memory_ids[i:i + batch_size]\n",
    "                    self.collection.delete(ids=batch)\n",
    "                \n",
    "                logger.info(f\"Forgetting policy removed {len(memory_ids)} memories\")\n",
    "            \n",
    "            return len(memory_ids)\n",
    "\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            logger.error(f\"Forgetting policy failed: {e}\")\n",
    "            return 0\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    def summarize_conversations(self, llm: BaseLanguageModel, min_messages: int = 10) -> bool:\n",
    "        \"\"\"Summarize old conversations\"\"\"\n",
    "        session = self.SessionLocal()\n",
    "        try:\n",
    "            # Find conversation threads to summarize\n",
    "            cutoff_date = datetime.now(timezone.utc) - timedelta(days=3)\n",
    "            \n",
    "            conversations = session.execute(\n",
    "                text(\"\"\"\n",
    "                    SELECT session_id, array_agg(id ORDER BY created_at) as memory_ids,\n",
    "                           array_agg(content ORDER BY created_at) as contents\n",
    "                    FROM agent_memories\n",
    "                    WHERE agent_id = :agent_id\n",
    "                        AND memory_type = 'conversation'\n",
    "                        AND is_summarized = false\n",
    "                        AND created_at < :cutoff\n",
    "                    GROUP BY session_id\n",
    "                    HAVING COUNT(*) >= :min_messages\n",
    "                    LIMIT 5\n",
    "                \"\"\"),\n",
    "                {\n",
    "                    'agent_id': self.agent_id,\n",
    "                    'cutoff': cutoff_date,\n",
    "                    'min_messages': min_messages\n",
    "                }\n",
    "            ).fetchall()\n",
    "\n",
    "            if not conversations:\n",
    "                return False\n",
    "\n",
    "            for conv in conversations:\n",
    "                # Create summary\n",
    "                dialogue = \"\\n\".join(conv.contents[:50])\n",
    "                prompt = f\"\"\"Summarize this conversation in 200 words or less, preserving key facts and decisions:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "                try:\n",
    "                    response = llm.invoke(prompt)\n",
    "                    summary = response.content if hasattr(response, 'content') else str(response)\n",
    "                    \n",
    "                    # Create summary memory\n",
    "                    summary_id = self.create_memory(\n",
    "                        session_id=conv.session_id,\n",
    "                        memory_type='summary',\n",
    "                        content=summary,\n",
    "                        importance=0.7,\n",
    "                        is_persistent=True,\n",
    "                        tags=['conversation_summary', 'auto_generated'],\n",
    "                        meta_data={\n",
    "                            'source_memory_ids': [str(id) for id in conv.memory_ids[:50]],\n",
    "                            'message_count': len(conv.memory_ids)\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Mark originals as summarized\n",
    "                    session.execute(\n",
    "                        text(\"\"\"\n",
    "                            UPDATE agent_memories\n",
    "                            SET is_summarized = true\n",
    "                            WHERE id = ANY(:memory_ids)\n",
    "                        \"\"\"),\n",
    "                        {'memory_ids': conv.memory_ids[:50]}\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to summarize conversation: {e}\")\n",
    "                    continue\n",
    "\n",
    "            session.commit()\n",
    "            logger.info(f\"Summarized {len(conversations)} conversations\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            logger.error(f\"Conversation summarization failed: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "    def create_memory_tools(self) -> List[BaseTool]:\n",
    "        \"\"\"Create memory tools for LangChain agents\"\"\"\n",
    "        \n",
    "        @tool(args_schema=RememberFactInput)\n",
    "        def remember_fact(content: str, importance: float = 0.5, \n",
    "                         tags: List[str] = None, is_persistent: bool = True) -> str:\n",
    "            \"\"\"Store important information in memory with appropriate persistence.\"\"\"\n",
    "            try:\n",
    "                memory_type = \"fact\" if is_persistent else \"temporary\"\n",
    "                \n",
    "                memory_id = self.create_memory(\n",
    "                    session_id=\"current_session\",\n",
    "                    memory_type=memory_type,\n",
    "                    content=content,\n",
    "                    importance=importance,\n",
    "                    tags=tags or [],\n",
    "                    is_persistent=is_persistent\n",
    "                )\n",
    "                return f\"Successfully stored memory (ID: {memory_id}). Persistence: {is_persistent}\"\n",
    "            except Exception as e:\n",
    "                return f\"Failed to store memory: {str(e)}\"\n",
    "\n",
    "        @tool(args_schema=RecallMemoriesInput)\n",
    "        def recall_memories(query: str, memory_types: List[str] = None, \n",
    "                           limit: int = 5, min_score: float = 0.3) -> str:\n",
    "            \"\"\"Search memories using semantic similarity.\"\"\"\n",
    "            try:\n",
    "                results = self.semantic_search(\n",
    "                    query=query,\n",
    "                    memory_types=memory_types,\n",
    "                    limit=limit,\n",
    "                    min_score=min_score\n",
    "                )\n",
    "                \n",
    "                if not results:\n",
    "                    return \"No relevant memories found.\"\n",
    "                \n",
    "                formatted = []\n",
    "                for mem in results:\n",
    "                    formatted.append(\n",
    "                        f\"[{mem['memory_type'].upper()}] {mem['content'][:150]}... \"\n",
    "                        f\"(Score: {mem['similarity_score']:.3f}, Importance: {mem['importance']:.2f})\"\n",
    "                    )\n",
    "                \n",
    "                return \"\\n\\n\".join(formatted)\n",
    "            except Exception as e:\n",
    "                return f\"Memory search failed: {str(e)}\"\n",
    "\n",
    "        @tool(args_schema=ReflectPerformanceInput)\n",
    "        def reflect_on_performance(topic: str, insights: List[str] = None, \n",
    "                                  action_items: List[str] = None) -> str:\n",
    "            \"\"\"Record reflections and insights for continuous improvement.\"\"\"\n",
    "            try:\n",
    "                reflection_content = f\"Reflection on {topic}\"\n",
    "                if insights:\n",
    "                    reflection_content += f\"\\nInsights: {', '.join(insights)}\"\n",
    "                if action_items:\n",
    "                    reflection_content += f\"\\nActions: {', '.join(action_items)}\"\n",
    "                \n",
    "                memory_id = self.create_memory(\n",
    "                    session_id=\"current_session\",\n",
    "                    memory_type=\"reflection\",\n",
    "                    content=reflection_content,\n",
    "                    importance=0.8,\n",
    "                    tags=[\"reflection\", \"self_improvement\"],\n",
    "                    is_persistent=True,\n",
    "                    meta_data={\n",
    "                        \"insights\": insights or [],\n",
    "                        \"action_items\": action_items or []\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                return f\"Reflection recorded (ID: {memory_id})\"\n",
    "            except Exception as e:\n",
    "                return f\"Failed to record reflection: {str(e)}\"\n",
    "\n",
    "        return [remember_fact, recall_memories, reflect_on_performance]\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get memory statistics from both stores\"\"\"\n",
    "        session = self.SessionLocal()\n",
    "        try:\n",
    "            # PostgreSQL stats\n",
    "            pg_stats = session.execute(\n",
    "                text(\"\"\"\n",
    "                    SELECT \n",
    "                        COUNT(*) as total_memories,\n",
    "                        COUNT(*) FILTER (WHERE is_persistent) as persistent_count,\n",
    "                        COUNT(*) FILTER (WHERE NOT is_persistent) as temporary_count,\n",
    "                        AVG(importance) as avg_importance,\n",
    "                        AVG(confidence) as avg_confidence,\n",
    "                        SUM(access_count) as total_accesses\n",
    "                    FROM agent_memories\n",
    "                    WHERE agent_id = :agent_id\n",
    "                \"\"\"),\n",
    "                {'agent_id': self.agent_id}\n",
    "            ).fetchone()\n",
    "\n",
    "            # ChromaDB stats\n",
    "            collection_count = self.collection.count()\n",
    "\n",
    "            return {\n",
    "                'agent_id': self.agent_id,\n",
    "                'postgresql': {\n",
    "                    'total_memories': pg_stats.total_memories,\n",
    "                    'persistent_count': pg_stats.persistent_count,\n",
    "                    'temporary_count': pg_stats.temporary_count,\n",
    "                    'avg_importance': float(pg_stats.avg_importance or 0),\n",
    "                    'avg_confidence': float(pg_stats.avg_confidence or 0),\n",
    "                    'total_accesses': int(pg_stats.total_accesses or 0)\n",
    "                },\n",
    "                'chromadb': {\n",
    "                    'vector_count': collection_count\n",
    "                },\n",
    "                'timestamp': datetime.now(timezone.utc).isoformat()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get statistics: {e}\")\n",
    "            return {'error': str(e)}\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "# LangGraph State\n",
    "class AgenticGraphState(TypedDict):\n",
    "    \"\"\"State for agentic workflows\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    user_input: str\n",
    "    ai_response: str\n",
    "    session_id: str\n",
    "    agent_id: str\n",
    "    tools_output: List[Dict[str, Any]]\n",
    "    should_continue: bool\n",
    "\n",
    "class ToolExecutingAgent:\n",
    "    \"\"\"Agent that executes tools through LangChain\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: BaseLanguageModel, memory_manager: ChromaDBMemoryManager):\n",
    "        self.llm = llm\n",
    "        self.memory_manager = memory_manager\n",
    "        self.tools = memory_manager.create_memory_tools()\n",
    "        \n",
    "        # Create agent with tools\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an AI assistant with persistent memory capabilities.\n",
    "            \n",
    "You have access to these tools:\n",
    "- remember_fact: Store important information (use is_persistent=True for facts, False for temporary info)\n",
    "- recall_memories: Search your memory for relevant information\n",
    "- reflect_on_performance: Record insights and learnings\n",
    "\n",
    "Use these tools proactively to:\n",
    "1. Store important facts and user preferences as persistent memories\n",
    "2. Store conversation context as temporary memories\n",
    "3. Always search your memory before answering questions\n",
    "4. Reflect on your performance to improve\n",
    "\n",
    "Memory persistence guidelines:\n",
    "- User preferences, facts, knowledge → is_persistent=True\n",
    "- Conversation, observations, scratch notes → is_persistent=False\"\"\"),\n",
    "            MessagesPlaceholder(\"messages\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "        ])\n",
    "        \n",
    "        self.agent = create_openai_tools_agent(llm, self.tools, self.prompt)\n",
    "        self.agent_executor = AgentExecutor(\n",
    "            agent=self.agent, \n",
    "            tools=self.tools, \n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True,\n",
    "            max_iterations=5\n",
    "        )\n",
    "        \n",
    "        # Create workflow\n",
    "        self.workflow = self._create_workflow()\n",
    "    \n",
    "    def _create_workflow(self) -> StateGraph:\n",
    "        \"\"\"Create workflow with tool execution\"\"\"\n",
    "        workflow = StateGraph(AgenticGraphState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"agent\", self._run_agent)\n",
    "        workflow.add_node(\"save_interaction\", self._save_interaction)\n",
    "        workflow.add_node(\"apply_policies\", self._apply_policies)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        workflow.add_edge(\"agent\", \"save_interaction\")\n",
    "        workflow.add_edge(\"save_interaction\", \"apply_policies\")\n",
    "        workflow.add_edge(\"apply_policies\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def _run_agent(self, state: AgenticGraphState) -> AgenticGraphState:\n",
    "        \"\"\"Run the agent with tools\"\"\"\n",
    "        try:\n",
    "            # Execute agent\n",
    "            result = self.agent_executor.invoke({\n",
    "                \"input\": state[\"user_input\"],\n",
    "                \"messages\": state.get(\"messages\", [])\n",
    "            })\n",
    "            \n",
    "            state[\"ai_response\"] = result[\"output\"]\n",
    "            state[\"tools_output\"] = result.get(\"intermediate_steps\", [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Agent execution failed: {e}\")\n",
    "            state[\"ai_response\"] = f\"I encountered an error: {str(e)}\"\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def _save_interaction(self, state: AgenticGraphState) -> AgenticGraphState:\n",
    "        \"\"\"Save the interaction to memory\"\"\"\n",
    "        try:\n",
    "            # Save user input as temporary conversation memory\n",
    "            self.memory_manager.create_memory(\n",
    "                session_id=state[\"session_id\"],\n",
    "                memory_type=\"conversation\",\n",
    "                content=f\"Human: {state['user_input']}\",\n",
    "                importance=0.5,\n",
    "                is_persistent=False,\n",
    "                tags=[\"conversation\", \"user_message\"]\n",
    "            )\n",
    "            \n",
    "            # Save AI response as temporary conversation memory\n",
    "            self.memory_manager.create_memory(\n",
    "                session_id=state[\"session_id\"],\n",
    "                memory_type=\"conversation\", \n",
    "                content=f\"Assistant: {state['ai_response']}\",\n",
    "                importance=0.5,\n",
    "                is_persistent=False,\n",
    "                tags=[\"conversation\", \"ai_response\"]\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save interaction: {e}\")\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def _apply_policies(self, state: AgenticGraphState) -> AgenticGraphState:\n",
    "        \"\"\"Apply memory policies\"\"\"\n",
    "        try:\n",
    "            # Apply forgetting policy\n",
    "            deleted = self.memory_manager.apply_forgetting_policy()\n",
    "            if deleted > 0:\n",
    "                logger.info(f\"Cleaned up {deleted} expired memories\")\n",
    "            \n",
    "            # Summarize conversations if LLM available\n",
    "            if hasattr(self, 'llm'):\n",
    "                self.memory_manager.summarize_conversations(self.llm)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to apply policies: {e}\")\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def run(self, user_input: str, session_id: str) -> str:\n",
    "        \"\"\"Run the agent\"\"\"\n",
    "        initial_state = {\n",
    "            \"user_input\": user_input,\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_id\": self.memory_manager.agent_id,\n",
    "            \"messages\": [],\n",
    "            \"tools_output\": [],\n",
    "            \"should_continue\": True,\n",
    "            \"ai_response\": \"\"\n",
    "        }\n",
    "        \n",
    "        result = self.workflow.invoke(initial_state)\n",
    "        return result[\"ai_response\"]\n",
    "\n",
    "# Utility functions for production deployment\n",
    "class ChromaDBDeployment:\n",
    "    \"\"\"Production deployment utilities for ChromaDB + PostgreSQL\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_production_agent(\n",
    "        database_url: str,\n",
    "        agent_id: str,\n",
    "        llm: BaseLanguageModel,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        chroma_persist_dir: str = \"./chroma_db\",\n",
    "        chroma_host: Optional[str] = None,\n",
    "        chroma_port: Optional[int] = None\n",
    "    ) -> ToolExecutingAgent:\n",
    "        \"\"\"Create production-ready agent with ChromaDB\"\"\"\n",
    "        \n",
    "        # Initialize hybrid memory manager\n",
    "        memory_manager = ChromaDBMemoryManager(\n",
    "            database_url=database_url,\n",
    "            agent_id=agent_id,\n",
    "            embedding_model_name=embedding_model,\n",
    "            chroma_persist_directory=chroma_persist_dir,\n",
    "            chroma_host=chroma_host,\n",
    "            chroma_port=chroma_port\n",
    "        )\n",
    "        \n",
    "        # Create tool-executing agent\n",
    "        agent = ToolExecutingAgent(llm, memory_manager)\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_monitoring():\n",
    "        \"\"\"SQL views for monitoring the hybrid system\"\"\"\n",
    "        return \"\"\"\n",
    "        -- Memory distribution view\n",
    "        CREATE OR REPLACE VIEW memory_distribution AS\n",
    "        SELECT \n",
    "            agent_id,\n",
    "            memory_type,\n",
    "            COUNT(*) as count,\n",
    "            COUNT(*) FILTER (WHERE is_persistent) as persistent,\n",
    "            COUNT(*) FILTER (WHERE NOT is_persistent) as temporary,\n",
    "            AVG(importance) as avg_importance,\n",
    "            SUM(access_count) as total_accesses,\n",
    "            MAX(last_accessed) as last_accessed\n",
    "        FROM agent_memories\n",
    "        GROUP BY agent_id, memory_type\n",
    "        ORDER BY agent_id, count DESC;\n",
    "        \n",
    "        -- Memory lifecycle view\n",
    "        CREATE OR REPLACE VIEW memory_lifecycle AS\n",
    "        SELECT \n",
    "            agent_id,\n",
    "            DATE(created_at) as date,\n",
    "            COUNT(*) as created,\n",
    "            COUNT(*) FILTER (WHERE is_persistent) as persistent_created,\n",
    "            COUNT(*) FILTER (WHERE expires_at < NOW()) as expired,\n",
    "            COUNT(*) FILTER (WHERE is_summarized) as summarized\n",
    "        FROM agent_memories\n",
    "        GROUP BY agent_id, DATE(created_at)\n",
    "        ORDER BY agent_id, date DESC;\n",
    "        \n",
    "        -- Agent activity view\n",
    "        CREATE OR REPLACE VIEW agent_activity AS\n",
    "        SELECT \n",
    "            a.agent_id,\n",
    "            COUNT(DISTINCT a.session_id) as unique_sessions,\n",
    "            COUNT(a.id) as total_memories,\n",
    "            COUNT(r.id) as total_reflections,\n",
    "            COUNT(t.id) as total_tasks,\n",
    "            MAX(a.created_at) as last_activity\n",
    "        FROM agent_memories a\n",
    "        LEFT JOIN agent_reflections r ON a.agent_id = r.agent_id\n",
    "        LEFT JOIN agent_tasks t ON a.agent_id = t.agent_id\n",
    "        GROUP BY a.agent_id;\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def backup_vectors(memory_manager: ChromaDBMemoryManager, backup_path: str):\n",
    "        \"\"\"Backup ChromaDB vectors to disk\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        # Get all memories\n",
    "        all_data = memory_manager.collection.get()\n",
    "        \n",
    "        backup_data = {\n",
    "            'ids': all_data['ids'],\n",
    "            'embeddings': all_data.get('embeddings'),\n",
    "            'metadatas': all_data['metadatas'],\n",
    "            'documents': all_data['documents'],\n",
    "            'timestamp': datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(backup_path, 'wb') as f:\n",
    "            pickle.dump(backup_data, f)\n",
    "        \n",
    "        logger.info(f\"Backed up {len(all_data['ids'])} vectors to {backup_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def restore_vectors(memory_manager: ChromaDBMemoryManager, backup_path: str):\n",
    "        \"\"\"Restore ChromaDB vectors from backup\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(backup_path, 'rb') as f:\n",
    "            backup_data = pickle.load(f)\n",
    "        \n",
    "        # Restore in batches\n",
    "        batch_size = 100\n",
    "        ids = backup_data['ids']\n",
    "        \n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            batch_end = min(i + batch_size, len(ids))\n",
    "            \n",
    "            memory_manager.collection.add(\n",
    "                ids=ids[i:batch_end],\n",
    "                embeddings=backup_data['embeddings'][i:batch_end] if backup_data.get('embeddings') else None,\n",
    "                metadatas=backup_data['metadatas'][i:batch_end],\n",
    "                documents=backup_data['documents'][i:batch_end]\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Restored {len(ids)} vectors from {backup_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67f5d893b56a32c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T05:53:19.704398Z",
     "start_time": "2025-07-21T05:53:18.775903Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model      = \"deepseek-reasoner\",\n",
    "    temperature= 0.0,\n",
    "    max_tokens = 1024*2,            \n",
    "    api_key  = \"\",             # fallback if you don’t use env‑var\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629bb82a3a1b5351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T05:53:26.344026Z",
     "start_time": "2025-07-21T05:53:20.550434Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:06:04,313 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋 If you\\'re just testing things out with a classic \"hello world,\" welcome! 😊  \\nIs there something specific you\\'d like help with today? Whether it\\'s coding, learning, or solving a problem—I\\'m here to assist! 💻📚✨  \\n\\nOr if you meant to run code:  \\n```python  \\nprint(\"Hello, World!\")  \\n```  \\n**Output**:  \\n```  \\nHello, World!  \\n```  \\n\\nLet me know how I can help! 🙌', additional_kwargs={'refusal': None, 'reasoning_content': 'We are given the input \"hello world\". This is a simple string.\\n We are to output a response. Since the user just said \"hello world\", we can respond with a friendly greeting.\\n Let\\'s respond with \"Hello! How can I assist you today?\"'}, response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 7, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 53, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 7}, 'model_name': 'deepseek-reasoner', 'system_fingerprint': 'fp_393bca965e_prod0623_fp8_kvcache', 'id': 'a0b579cb-6f3f-41c9-92bf-92c71c28decb', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--71daa4c9-aab1-4c17-8211-a7655d1085ea-0', usage_metadata={'input_tokens': 7, 'output_tokens': 158, 'total_tokens': 165, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 53}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a79f7ae-6132-44b8-84c5-1f107e5c7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"postgresql://username:password@localhost:5432/agentic_memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda555ce-c0a3-4d19-8691-7d65dcd0f32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:06:45,596 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps\n",
      "2025-07-21 17:06:45,597 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-07-21 17:06:50,147 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-07-21 17:06:53,550 - __main__ - INFO - Created new ChromaDB collection: agent_agent_001_memories\n",
      "2025-07-21 17:06:53,600 - __main__ - INFO - All PostgreSQL tables created/verified\n",
      "2025-07-21 17:06:53,609 - __main__ - INFO - ChromaDBMemoryManager initialized for agent: agent_001\n"
     ]
    }
   ],
   "source": [
    "agent = ChromaDBDeployment.create_production_agent(\n",
    "    database_url=db_url,\n",
    "    agent_id=\"agent_001\",\n",
    "    llm=llm,\n",
    "    chroma_persist_dir=\"./chroma_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe05bd5d-ab40-42a4-9783-4a133d241d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:07:30,591 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `remember_fact` with `{'content': \"User's name is Batman\", 'tags': ['user_preference', 'identity'], 'is_persistent': True}`\n",
      "responded: It's a pleasure to meet you, Batman! I'll make sure to remember your name for future interactions. I'll store this important detail in my persistent memory.\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c9690c7c77410a8f3be71e59f389e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:07:41,050 - __main__ - INFO - Created persistent memory 6c1248fd-d98b-4fb4-855b-b6dedbe6ebe5 in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mSuccessfully stored memory (ID: 6c1248fd-d98b-4fb4-855b-b6dedbe6ebe5). Persistence: True\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:07:41,244 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mGreat to meet you, Batman! I've stored your name in my memory so I can address you properly in our future conversations. What can I assist you with today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b2f1027b7f445e8bbc0fc5c9f957c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:07:49,293 - __main__ - INFO - Created temporary memory 0974c43e-35a3-4f23-99ac-10d2cde4f655 in both stores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e9c07311654849995e68c0b6963456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:07:49,317 - __main__ - INFO - Created temporary memory 8942883c-b7ff-40e0-87f7-fae9884e7e26 in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Great to meet you, Batman! I've stored your name in my memory so I can address you properly in our future conversations. What can I assist you with today?\n"
     ]
    }
   ],
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "    \n",
    "# First interaction - agent should remember\n",
    "response1 = agent.run(\n",
    "    \"My name is batman, nice to meet you\",\n",
    "    session_id\n",
    ")\n",
    "print(\"Response 1:\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ca1af0-fccd-4a5a-9a67-ec39fcda6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:08:30,799 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `remember_fact` with `{'content': \"The user's actual name is Bruce Wayne\", 'tags': ['user identity', 'personal information'], 'is_persistent': True, 'importance': 0.9}`\n",
      "responded: I'll remember that for our future conversations, Bruce Wayne. Let me store this important personal detail.\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360f35edfd5e4f069091ca84dd08931e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:09:00,212 - __main__ - INFO - Created persistent memory a11ea009-cca0-483b-8ee1-f144ad5f22ff in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mSuccessfully stored memory (ID: a11ea009-cca0-483b-8ee1-f144ad5f22ff). Persistence: True\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:09:00,429 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mGot it, Bruce Wayne. I've stored your actual name as a persistent memory with high importance. I'll address you as Bruce going forward, unless you prefer another form of address. \n",
      "\n",
      "Is there anything specific you'd like to discuss today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceac06950f73425fbf03598869290c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:09:13,145 - __main__ - INFO - Created temporary memory 21a1e434-a126-47b7-8beb-a6796cfe0487 in both stores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0d69f777354f528a060a9c35cd300f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:09:13,195 - __main__ - INFO - Created temporary memory 85faaca6-5cd7-41ca-8285-f329e13ac2a9 in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Got it, Bruce Wayne. I've stored your actual name as a persistent memory with high importance. I'll address you as Bruce going forward, unless you prefer another form of address. \n",
      "\n",
      "Is there anything specific you'd like to discuss today?\n"
     ]
    }
   ],
   "source": [
    "response1 = agent.run(\n",
    "    \"My actual name is bruce wayne\",\n",
    "    session_id\n",
    ")\n",
    "print(\"Response 1:\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800b07e4-c920-407f-8d88-86ceed5ae1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:11:16,882 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `recall_memories` with `{'query': \"Batman's approach to helping in dangerous situations, difference between Batman and Bruce Wayne personas\"}`\n",
      "responded: I'll help you think through this scenario step by step, drawing on Batman's principles and practical considerations. First, let me recall relevant information about Batman's methodology:\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ea1f0e3a0c465d85ba7f6b05d96a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m[FACT] The user's actual name is Bruce Wayne... (Score: 0.448, Importance: 0.90)\n",
      "\n",
      "[FACT] User's name is Batman... (Score: 0.515, Importance: 0.50)\n",
      "\n",
      "[CONVERSATION] Human: My name is batman, nice to meet you... (Score: 0.437, Importance: 0.50)\n",
      "\n",
      "[CONVERSATION] Human: My actual name is bruce wayne... (Score: 0.419, Importance: 0.50)\n",
      "\n",
      "[CONVERSATION] Assistant: Great to meet you, Batman! I've stored your name in my memory so I can address you properly in our future conversations. What can I assist ... (Score: 0.348, Importance: 0.50)\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:12:22,335 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mBased on our previous conversations and core Batman principles, here's my analysis:\n",
      "\n",
      "### Key Considerations:\n",
      "1. **Identity Purpose** (from your history):\n",
      "   - **Batman**: Symbol of fear for criminals, hope for citizens. Maintains anonymity.\n",
      "   - **Bruce Wayne**: Public facade for philanthropy and social access.\n",
      "\n",
      "2. **Alley Situation Dynamics**:\n",
      "   - Requires immediate intimidation/combat capability\n",
      "   - High risk of criminal elements\n",
      "   - Requires anonymity to protect your dual identity\n",
      "\n",
      "### Recommendation: \n",
      "**Go as Batman**. Reasons:\n",
      "- 🦇 Instills immediate fear in potential attackers\n",
      "- 🛡️ Full access to tactical gear and combat training\n",
      "- 🌑 Maintains separation between your identities\n",
      "- 💥 Aligns with your mission: \"A symbol can be incorruptible\"\n",
      "\n",
      "### Tactical Advice:\n",
      "1. Scout from rooftops first\n",
      "2. Use smoke pellets for controlled engagement\n",
      "3. Prioritize victim extraction over pursuit\n",
      "4. Disappear before authorities arrive\n",
      "\n",
      "Would you like me to elaborate on any aspect of this approach?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220d4a7764a5413ab852e8721d42913d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:12:54,306 - __main__ - INFO - Created temporary memory 91d6d713-d713-4482-b95d-ea089d23d4f1 in both stores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899c6e379aea463ebc965770d5065de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:12:54,337 - __main__ - INFO - Created temporary memory 99708e4d-fbb8-4ff8-9981-12b790cf4f77 in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Based on our previous conversations and core Batman principles, here's my analysis:\n",
      "\n",
      "### Key Considerations:\n",
      "1. **Identity Purpose** (from your history):\n",
      "   - **Batman**: Symbol of fear for criminals, hope for citizens. Maintains anonymity.\n",
      "   - **Bruce Wayne**: Public facade for philanthropy and social access.\n",
      "\n",
      "2. **Alley Situation Dynamics**:\n",
      "   - Requires immediate intimidation/combat capability\n",
      "   - High risk of criminal elements\n",
      "   - Requires anonymity to protect your dual identity\n",
      "\n",
      "### Recommendation: \n",
      "**Go as Batman**. Reasons:\n",
      "- 🦇 Instills immediate fear in potential attackers\n",
      "- 🛡️ Full access to tactical gear and combat training\n",
      "- 🌑 Maintains separation between your identities\n",
      "- 💥 Aligns with your mission: \"A symbol can be incorruptible\"\n",
      "\n",
      "### Tactical Advice:\n",
      "1. Scout from rooftops first\n",
      "2. Use smoke pellets for controlled engagement\n",
      "3. Prioritize victim extraction over pursuit\n",
      "4. Disappear before authorities arrive\n",
      "\n",
      "Would you like me to elaborate on any aspect of this approach?\n"
     ]
    }
   ],
   "source": [
    "response1 = agent.run(\n",
    "    \"If someone call me for help in a  back alley should go as a batman symbole of justice or bruce wayne\",\n",
    "    session_id\n",
    ")\n",
    "print(\"Response 1:\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "633b56e8-332c-4056-9556-ce7a97c72f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:13:37,864 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `remember_fact` with `{'content': \"User's favorite programming language is Python\", 'tags': ['user_preference', 'programming'], 'is_persistent': True, 'importance': 0.8}`\n",
      "responded: I'll remember your preferences! Let me store these details:\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fcf69ddb4c43f5a9f12398e83f2230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:13:55,677 - __main__ - INFO - Created persistent memory 1af90a08-0f02-418c-b5c2-d2dbc722d31e in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mSuccessfully stored memory (ID: 1af90a08-0f02-418c-b5c2-d2dbc722d31e). Persistence: True\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `remember_fact` with `{'content': 'User prefers tabs over spaces for indentation', 'tags': ['user_preference', 'coding_style'], 'is_persistent': True, 'importance': 0.7}`\n",
      "responded: I'll remember your preferences! Let me store these details:\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7884acfeea70499f854510a15c6b3e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:13:55,704 - __main__ - INFO - Created persistent memory 19a1fa3d-bd1e-4853-a20c-ea746c019581 in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mSuccessfully stored memory (ID: 19a1fa3d-bd1e-4853-a20c-ea746c019581). Persistence: True\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:13:55,922 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mGot it! I've noted your preferences:\n",
      "1. Your favorite programming language is Python\n",
      "2. You prefer tabs over spaces for indentation\n",
      "\n",
      "I'll keep these in mind whenever we discuss programming topics. Let me know if you'd like to add any other preferences!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5808dd977949b69aa3a2419a330f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:14:03,691 - __main__ - INFO - Created temporary memory c9660aee-7b4f-4326-918b-f45a845664e7 in both stores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8701190895415bb05742d3d3450ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:14:03,721 - __main__ - INFO - Created temporary memory de5e7986-9e6f-4793-9931-72b6cdb3cca4 in both stores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Got it! I've noted your preferences:\n",
      "1. Your favorite programming language is Python\n",
      "2. You prefer tabs over spaces for indentation\n",
      "\n",
      "I'll keep these in mind whenever we discuss programming topics. Let me know if you'd like to add any other preferences!\n"
     ]
    }
   ],
   "source": [
    "response1 = agent.run(\n",
    "        \"My favorite programming language is Python and I prefer tabs over spaces.\",\n",
    "        session_id\n",
    "    )\n",
    "print(\"Response 1:\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d5e33-88ac-4f51-a9eb-08d6efb4090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
